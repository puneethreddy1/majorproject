# -*- coding: utf-8 -*-
"""diabetes-health-indicators-classfication-project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R3dIzuDQBCZLX1dDVYaYtVM5m942vXY5

# Import Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
#Python libraries
#Classic,data manipulation 
import numpy as np
import pandas as pd
# Plots
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")
pd.set_option("display.max_rows",None)
from sklearn import preprocessing 
from sklearn.metrics import precision_score, recall_score, precision_recall_curve,f1_score, fbeta_score,accuracy_score
# %matplotlib inline
import seaborn as sns
from sklearn.model_selection import KFold,StratifiedKFold
from sklearn.ensemble import StackingClassifier

from imblearn.over_sampling import SMOTE
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler 
from sklearn.metrics import fbeta_score, make_scorer
from sklearn.model_selection import cross_validate
from sklearn.metrics import roc_curve

from xgboost import XGBClassifier
from xgboost import XGBRegressor
# Dataprep
# Modeling 
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.model_selection import cross_val_score 
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier,VotingClassifier
from imblearn.combine import SMOTEENN #resampling
from sklearn import datasets, linear_model, metrics
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
#Import pipeline to allow you to do multiple steps at once
from imblearn.pipeline import Pipeline, make_pipeline

"""# Load Dataset """

df= pd.read_csv("C:/CODES/MAJORPROJECT/diabetes_012_health_indicators_BRFSS2015.csv")

"""# Explore Dataset"""

df.info()

df.shape

df.sample(100)

df.columns

# to rename the some columns important of dataset
df.rename(columns={'Diabetes_012': 'Diabetes_Type'}, inplace=True)
df.head()

"""# Cleaning Dataset"""

df.isna().sum() # No missing value

df.BMI.unique()

"""Would you say that in general your health is:scale 1-5:
> 1 = excellent 

> 2 = very good 

> 3 = good 

> 4 = fair 

> 5 = poor

"""

df.GenHlth.unique()

"""Mental health scale:

> scale 1-30 days .
"""

df.MentHlth.unique()

"""Now thinking about your physical health, for how many days during the past 30 days 
was your physical health not good?

> scale 1-30 days 
"""

df.PhysHlth.unique()

"""13-level age categorys:(_AGEG5YR see codebook)
> 1 = 18-24 

> 9 = 60-64 

> 13 = 80 or older 

"""

df.Age.unique()

"""Education level (EDUCA see codebook) scale 1-6:
> 1 = Never attended school or only kindergarten 

> 2 = Grades 1 through 8 (Elementary)

> 3 = Grades 9 throug 11 (Some high school)

> 4 = Grade 12 or GED (High school graduate)

> 5 = College 1 year to 3 years (Some college or technical school) 

> 6 = College 4 years or more (College graduate)
"""

df.Education.unique()

"""Income scale 1-8:
> 1 = less than $10,000 

> 5 = less than $35,000 

> 8 = $75,000 or more


"""

df.Income.unique()

df.Diabetes_Type.unique()

Diabetes=df['Diabetes_Type']
Diabetes.value_counts()

df['Diabetes_Type'].replace({2.0: 1.0},inplace = True)

Diabetes=df['Diabetes_Type']
Diabetes.value_counts()

"""# preprocessing

# Visualze the correlation
"""

corr = df.corr()
fig, ax = plt.subplots(figsize=(25,15)) 
sns.heatmap(corr,annot=True, cmap = "Blues", linewidth = 0.30)
plt.title("Correlation matrix of features")
plt.show()

df['Diabetes_Type'] = df['Diabetes_Type'].astype('int')

df['Diabetes']=df['Diabetes_Type']

df['Diabetes'] = df['Diabetes_Type'].map({0:'No Diabetes', 1:'Diabetes'})

Diabetes=df['Diabetes_Type']
Diabetes.value_counts()

diabetes_bp = df.groupby(['Diabetes_Type', 'HighBP']).size().reset_index(name = 'Count')
print(diabetes_bp)

df.columns

df['GH']=df['GenHlth']

df['GH'] = df['GH'].map({1:5, 2:4 ,3:3 ,4:2 , 5:1})

df.head()

df.isna().sum()

"""# Build models

## Experiment 1: Split data to training,validation and test set
"""

smaller_df=df.loc[:,['Diabetes_Type', 'HighBP', 'HighChol', 'BMI', 'HeartDiseaseorAttack','PhysActivity', 'GenHlth','MentHlth','PhysHlth','DiffWalk','Sex','Age']]
smaller_df1=df.loc[:,['Diabetes_Type', 'HighBP', 'HighChol', 'BMI', 'HeartDiseaseorAttack','PhysActivity', 'GenHlth','MentHlth','PhysHlth','DiffWalk','Sex','Age']]

df_train, df_test = train_test_split(smaller_df, test_size=0.20, random_state=0)
df_train, df_val = train_test_split(df_train, test_size=0.20, random_state=0)

x,y = df_train.drop(['Diabetes_Type'],axis=1),df_train['Diabetes_Type']

"""## Baseline Model"""

# The figure above display the correlation between the features and the target, for this i choose only these features
x_train1,y_train = df_train.drop(['Diabetes_Type'],axis=1),df_train['Diabetes_Type']
x_val1,y_val = df_val.drop(['Diabetes_Type'],axis=1),df_val['Diabetes_Type']
x_test1,y_test= df_test.drop(['Diabetes_Type'],axis=1),df_test['Diabetes_Type']

print(x_train1.shape)
print(x_val1.shape)
print(x_test1.shape)

"""## Scalling to give us fair distrubtion btw features"""

#For smaller df1
scaler = MinMaxScaler()
scaler.fit(x_train1)

x_train = scaler.transform(x_train1)
x_test= scaler.transform(x_test1)
x_val=scaler.transform(x_val1)

"""## Experiment 1-1: K-nearest Neighbors Classification without using cross vaidation"""

# create a dict to store the scores of each model
models_evalutions = {'Model':[],
                     'Accuracy':[],
                     'Precision':[],
                     'Recall':[], 
                     'F1 score':[]}

#model_names = ["knn_final", "lr_final","Dt_final","rf_final"]
#model_vars = [eval(n) for n in model_names]
#model_list = list(zip(model_names, model_vars))

x0_train=x_train.copy()
x0_val=x_val.copy()

# Using KNN (smaller df1) train on training set, and Test on testing set 
# knn = KNeighborsClassifier(n_neighbors=5)
# knn.fit(x0_train, y_train)
# print("The score for kNN without cross val and without using smote is")
# print("Training set: {:6.2f}%".format(100*knn.score(x0_train, y_train)))
# print("Validation set: {:6.2f}%".format(100*knn.score(x0_val, y_val)))
# print("Test set: {:6.2f}%".format(100*knn.score(x_test, y_test)))

#test the baseline model for smaller df
# #prediction
# val_pred=knn.predict(x0_val)
# #Accuracy
# confusion_hard = confusion_matrix( y_val, val_pred)
# accuracy = accuracy_score(y_val , val_pred)
# precision = precision_score(y_val , val_pred)
# recall = recall_score(y_val , val_pred)
# f1 = f1_score(y_val,val_pred) 
# print('\nKNN Accuracy for validation set=: {0:.4f}, \nprecision: {1:.4f}, \nrecall: {2:.4f},\
# \nF1: {3:.4f}'.format(accuracy, precision, recall, f1))

# cm = confusion_matrix(y_val, val_pred)
# class_label = ["No_Diabetes", "Diabetes"]
# df_cm = pd.DataFrame(cm, index = class_label, columns = class_label)
# sns.heatmap(df_cm, annot = True, fmt = "d",  cmap = "Blues" )
# plt.title('Confusion matrix for knn befoes balancing two classes and using cross validation', fontsize = 20); # title with fontsize 20

"""## Experiment 1-2: K-nearest Neighbors Classification with cross validation & Smote

Becouse of our target's labels imbalance we will use Smote to balance them with cross validation.
"""

# kf = KFold(n_splits=5, random_state=42, shuffle=True)

# knn = KNeighborsClassifier(n_neighbors=5)
# accuracy_score1=[]
# f1_score1 = []
# percision_score1 = []
# recall_score1 = []

# # enumerate the splits and summarize the distributions
# for train_ix, test_ix in kf.split(x, y):
#     # select rows
#     train_x, test_X = x.iloc[train_ix], x.iloc[test_ix]
#     train_y, test_y = y.iloc[train_ix], y.iloc[test_ix]
#     #print(train_X.shape, train_y.shape)
#     #print(test_X.shape, test_y.shape)
#     oversample = SMOTE(random_state = 0)
#     train_x, train_y = oversample.fit_resample(train_x, train_y)
#     #scores= cross_val_score(knn, test_X, test_y, cv=5, scoring='accuracy') 
#     knn.fit(train_x, train_y)
#     y_pred =knn.predict(test_X)
#     #score = f1_score(test_y,y_pred)
#     accuracy_score1.append(metrics.accuracy_score(test_y, y_pred))
#     percision_score1.append(metrics.precision_score(test_y, y_pred))
#     recall_score1.append(metrics.recall_score(test_y, y_pred))
#     f1_score1.append(metrics.f1_score(test_y, y_pred))


    
# print("kNN accuracy score: \t")
# print(sum(accuracy_score1) / len(accuracy_score1))
# print("----------------")    
# print("kNN score: \t")
# print(sum(f1_score1) / len(f1_score1))
# print("----------------")
# conf_mat3 = confusion_matrix(test_y, y_pred)
# print("kNN confusion matrix: \n",conf_mat3)
# print("----------------")
# print("KNN precision score")
# print(sum(percision_score1) / len(percision_score1))
# print("----------------")
# print("KNN recall_score")
# print(sum(recall_score1) / len(recall_score1))
# print("----------------")
# #print score for all evaluations

# models_evalutions['Model'].append("KNN after balance our target's labels")
# models_evalutions['Accuracy'].append(accuracy_score(test_y, y_pred))
# models_evalutions['Precision'].append(precision_score(test_y, y_pred))
# models_evalutions['Recall'].append(recall_score(test_y, y_pred))
# models_evalutions['F1 score'].append(f1_score(test_y, y_pred))

# knn_final = knn.n_neighbors

# cm =confusion_matrix(test_y, y_pred)
# class_label = ["No-Diabetes", "Diabetes"]
# df_cm = pd.DataFrame(cm, index = class_label, columns = class_label)
# sns.heatmap(df_cm, annot = True, fmt = "d", cmap='Blues')
# plt.title('Confusion matrix for knn with cross validation and Smote', fontsize = 20); # title with fontsize 20

"""## Expreiment 2: Logistic Regression Model with smote"""

x2_train=x_train.copy()
x2_val=x_val.copy()

# #Before balance classes
# lr=LogisticRegression()
# prams ={"penalty": [ 'l1', 'l2'],
#        "C": [0.5 , 0.7,0.8 , 1 , 2.0 , 3.0]}

# lr_cv= GridSearchCV(lr , param_grid=prams, n_jobs=-1 ,cv=10)
# lr_cv.fit(x2_train , y_train )

# print("Best params: ", lr_cv.best_params_)
# print("Best estimator: " ,lr_cv.best_estimator_)
# print("Best score: ", lr_cv.best_score_)

# print("Training Score before balance the labels:",lr_cv.score(x2_train, y_train))
# print("Validation Score before balance the labels:",lr_cv.score(x2_val, y_val))

# y_pred = lr_cv.predict(x2_val)
# print("\nLogistic Regression Accuracy=",accuracy_score(y_val, y_pred))
# print("Logistic Regression F1 score=",f1_score(y_val, y_pred))

# Best params:  {'C': 1, 'penalty': 'l2'}
# lr_cv.best_score_

lr_final = LogisticRegression(C=1,penalty="l2")
#lr_final

#lr_final.score(x2_train , y_train)

#lr_final.score(x_val , y_val)

"""The Class Imbalance classification divided into three:

> 1. Before model training: Resampling strategies (oversampling, undersampling)

> 2. During model training: Training with adjusted class weights

> 3. After model training: Adjusting the decision threshold (F1 optimization strategy)
"""

#experiment2-1 with random over sampling
# lg1 = LogisticRegression(C=1,penalty="l2")

# # randomly oversample positive samples
# ROS = RandomOverSampler(random_state=42)

# X_tr_rs, y_tr_rs = ROS.fit_resample(x2_train, y_train)

# lg1.fit(X_tr_rs, y_tr_rs)
# print("Training Score after balance the labels (RandomOverSampler):",lg1.score(X_tr_rs, y_tr_rs))
# print("Validation Score after balance the labels (RandomOverSampler)",lg1.score(x_val, y_val))
#model_eval(model3,X_test_std,y_test)

# #experiment2-2 whith random under sampling
# lg2 = LogisticRegression(C=1,penalty="l2")

# RUS = RandomUnderSampler(random_state=42)

# X_tr_us, y_tr_us = RUS.fit_resample(x2_train, y_train)

# lg2.fit(X_tr_us, y_tr_us)
# print("Training Score after balance the labels (RandomUnderSampler)",lg2.score(X_tr_us, y_tr_us))
# print("Validation Score after balance the labels (RandomUnderSampler):",lg2.score(x2_val, y_val))
#model_eval(lg2,X_test_std,y_test)

#experiment2-3 whith balanced weighted classes sampling
# lg3 = LogisticRegression(C=1,penalty="l2",class_weight='balanced')

# lg3.fit(x2_train, y_train)
# #y_pred=lg3.predict(x2_val)
# print("Training Score after Balanced class weights Logistic Regression",lg3.score(x2_train, y_train))
# print("Validation Score after Balanced class weights Logistic Regression:",lg3.score(x2_val, y_val))

#experiment2-4 whith Smote
lg = LogisticRegression(C=1,penalty="l2")

SMT = SMOTE(random_state=42)

X_tr_smt, y_tr_smt = SMT.fit_resample(x2_train, y_train)

lg.fit(X_tr_smt, y_tr_smt)
y_pred=lg.predict(x2_val)

print("Training Score after balance the labels (Smote):",lg.score(X_tr_smt, y_tr_smt))
print("Validation Score after balance the labels (Smote):",lg.score(x2_val, y_val))
#model_eval(model3,X_test_std,y_test)

"""the best result was for Smote """

# classification report for logisitic
print(classification_report(y_val, y_pred, digits=3, zero_division = 1))
acc_nb = accuracy_score(y_val, y_pred)
recall_nb = recall_score(y_val, y_pred, average="binary")
print("ACCURACY:",accuracy_score(y_val, y_pred))
print("RECALL:",recall_score(y_val, y_pred, average="binary"))

models_evalutions['Model'].append("LogisticRegression with Smote")
models_evalutions['Accuracy'].append(accuracy_score(y_val, y_pred))
models_evalutions['Precision'].append(precision_score(y_val, y_pred))
models_evalutions['Recall'].append(recall_score(y_val, y_pred))
models_evalutions['F1 score'].append(f1_score(y_val, y_pred))

# plotting confusion mtrix
cm = confusion_matrix(y_val, y_pred)
class_label = ["No-Diabetes", "Diabetes"]
df_cm = pd.DataFrame(cm, index = class_label, columns = class_label)
sns.heatmap(df_cm, annot = True, fmt = "d", cmap='Blues')
plt.title('Confusion matrix for LogisticRegression ', fontsize = 20); # title with fontsize 20

"""## Experiment 3: Decision Tree Classification"""

x3_train=x_train.copy()
x3_val=x_val.copy()

# finding the best parameters for the decision tree
# param_grid = {'criterion' :['gini', 'entropy'],'max_depth': [4, 6, 10, 12]}

# tree_clas = DecisionTreeClassifier(random_state=42)
# grid_search = GridSearchCV(estimator=tree_clas, param_grid=param_grid, cv=5, verbose=True, scoring = 'f1')
# grid_search.fit(x3_train, y_train)

# print(grid_search.best_estimator_)

# Dt_final = grid_search.best_estimator_
# Dt_final

# trying with entropy, since it didn't show in the previose step
tree = DecisionTreeClassifier(criterion='entropy',
                                     max_depth=10,
                                     max_features='auto',
                                     random_state=42)

tree.fit(x3_train,y_train)

print("Training Score In Decision Tree Classification:",tree.score(x3_train, y_train))
print("Validation Score In Decision Tree: Classification",tree.score(x2_val, y_val))
y_pred = tree.predict(x3_val)

print("DT Accuracy=",accuracy_score(y_val, y_pred))
print("DT F1 score=",f1_score(y_val, y_pred))

# classification report for Decision Tree 
print(classification_report(y_val, y_pred, digits=3, zero_division = 1))
acc_nb = accuracy_score(y_val, y_pred)
recall_nb = recall_score(y_val, y_pred, average="binary")
print("ACCURACY:",accuracy_score(y_val, y_pred))
print("RECALL:",recall_score(y_val, y_pred, average="binary"))

models_evalutions['Model'].append("Decision Tree Classification")
models_evalutions['Accuracy'].append(accuracy_score(y_val, y_pred))
models_evalutions['Precision'].append(precision_score(y_val, y_pred))
models_evalutions['Recall'].append(recall_score(y_val, y_pred))
models_evalutions['F1 score'].append(f1_score(y_val, y_pred))

# plotting confusion mtrix
cm = confusion_matrix(y_val, y_pred)
class_label = ["No-Diabetes", "Diabetes"]
df_cm = pd.DataFrame(cm, index = class_label, columns = class_label)
sns.heatmap(df_cm, annot = True, fmt = "d", cmap='Blues')
plt.title('Confusion matrix for Decision Tree classification', fontsize = 20); # title with fontsize 20

"""## Expreiment 5: Random Forest Classification """

x5_train=x_train.copy()
x5_val=x_val.copy()

# params = {
#     'n_estimators': [50, 100, 200],
#     'max_depth': [4, 6, 10, 12],
#     'random_state': [13]
# }

#kf = KFold(n_splits=5, random_state=42, shuffle=False)
# example_params = {
#         'n_estimators': 50,
#         'max_depth':4 ,
#         'random_state': 13
#     }

# imba_pipeline = make_pipeline(SMOTE(random_state=42),RandomForestClassifier(n_estimators=5, random_state=13))
# cross_val_score(imba_pipeline, x5_train, y_train, scoring='f1', cv=kf)

# new_params = {'randomforestclassifier__' + key: params[key] for key in params}
# grid_imba = GridSearchCV(imba_pipeline, param_grid=new_params, cv=kf, scoring='f1',return_train_score=True)
# grid_imba.fit(x5_train, y_train)

# print("Training Score In Random Forest Classification:",grid_imba.score(x5_train, y_train))
# print("Validation Score In Random Forest Classification:",grid_imba.score(x5_val, y_val))

# rf_best=grid_imba.best_params_
# rf_best

# Random Forest with best hyperparameter
rf_best = RandomForestClassifier(n_estimators=100,
                                 max_depth=12,
                                 random_state=13)
rf_best.fit(x5_train, y_train)
y_pred = rf_best.predict(x5_val)

print("Training Score In Random Forest Classification with best parameters:",rf_best.score(x5_train, y_train))
print("Validation Score In Random Forest Classification:with best parameters",rf_best.score(x5_val, y_val))

# classification report for Random forest 
print(classification_report(y_val, y_pred, digits=3, zero_division = 1))
acc_nb = accuracy_score(y_val, y_pred)
recall_nb = recall_score(y_val, y_pred, average="binary")
print("ACCURACY:",accuracy_score(y_val, y_pred))
print("RECALL:",recall_score(y_val, y_pred, average="binary"))

models_evalutions['Model'].append("RandomForestClassifier_best parameters")
models_evalutions['Accuracy'].append(accuracy_score(y_val, y_pred))
models_evalutions['Precision'].append(precision_score(y_val, y_pred))
models_evalutions['Recall'].append(recall_score(y_val, y_pred))
models_evalutions['F1 score'].append(f1_score(y_val, y_pred))

# plotting confusion mtrix
cm = confusion_matrix(y_val, y_pred)
class_label = ["No-Diabetes", "Diabetes"]
df_cm = pd.DataFrame(cm, index = class_label, columns = class_label)
sns.heatmap(df_cm, annot = True, fmt = "d", cmap='Blues');
plt.title('Confusion matrix for Random Forest classification with best parameters', fontsize = 20); # title with fontsize 20

"""XGBOOST"""

from xgboost import XGBClassifier
xgboost = XGBClassifier(eval_metric= 'error', learning_rate= 0.1)
xgboost.fit(x_train, y_train)
y_pred = xgboost.predict(x_val)
y_prob = xgboost.predict_proba(x_test)[:,1]
cm = confusion_matrix(y_val, y_pred)
print("XgBoost Accuracy Score: ",accuracy_score(y_val, y_pred))

#Plot CM 
plt.figure(figsize = (6, 6))
sns.heatmap(cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, 
            yticklabels = ['Healthy', 'Diabetic'], xticklabels = ['Predicted Healthy', 'Predicted Diabetic'])
plt.yticks(rotation = 0)
plt.show()

models_evalutions['Model'].append("xgboost")
models_evalutions['Accuracy'].append(accuracy_score(y_val, y_pred))
models_evalutions['Precision'].append(precision_score(y_val, y_pred))
models_evalutions['Recall'].append(recall_score(y_val, y_pred))
models_evalutions['F1 score'].append(f1_score(y_val, y_pred))

"""ADABOOST"""

from sklearn.ensemble import AdaBoostClassifier
adaboost = AdaBoostClassifier(algorithm= 'SAMME.R', learning_rate= 1, n_estimators= 200, random_state= 0)
adaboost.fit(x_train, y_train)
y_pred = adaboost.predict(x_val)
y_prob = adaboost.predict_proba(x_test)[:,1]
cm = confusion_matrix(y_val, y_pred)
print("ada Accuracy Score: ",accuracy_score(y_val, y_pred))
#Plot CM 
plt.figure(figsize = (6, 6))
sns.heatmap(cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, 
            yticklabels = ['Healthy', 'Diabetic'], xticklabels = ['Predicted Healthy', 'Predicted Diabetic'])
plt.yticks(rotation = 0)
plt.show()

models_evalutions['Model'].append("adaboost")
models_evalutions['Accuracy'].append(accuracy_score(y_val, y_pred))
models_evalutions['Precision'].append(precision_score(y_val, y_pred))
models_evalutions['Recall'].append(recall_score(y_val, y_pred))
models_evalutions['F1 score'].append(f1_score(y_val, y_pred))

"""SVM"""

from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
nb = GaussianNB()
nb.fit(x_train,y_train)
print("Naive Bayes val score: ",nb.score(x_val,y_val))
print("Naive Bayes Train score: ",nb.score(x_train,y_train))
y_pred_nb = nb.predict(x_val)

y_pred=nb.predict(x_val)
print(classification_report(y_val, y_pred, digits=3, zero_division = 1))
acc_nb = accuracy_score(y_val, y_pred)
recall_nb = recall_score(y_val, y_pred, average="binary")
print("ACCURACY:",accuracy_score(y_val, y_pred))
print("RECALL:",recall_score(y_val, y_pred, average="binary"))

models_evalutions['Model'].append("Naive-B")
models_evalutions['Accuracy'].append(accuracy_score(y_val, y_pred))
models_evalutions['Precision'].append(precision_score(y_val, y_pred))
models_evalutions['Recall'].append(recall_score(y_val, y_pred))
models_evalutions['F1 score'].append(f1_score(y_val, y_pred))

"""## Experiment 6-1: Ensembling with Voting"""

x6_train=x_train.copy()
x6_val=x_val.copy()

model_names = ["rf_best","tree","xgboost","adaboost"]

model_vars = [eval(n) for n in model_names]
model_list = list(zip(model_names, model_vars))

model_names

model_list

for model_name in model_names:
    curr_model = eval(model_name)
    print(f'{model_name} score: {curr_model.score(x_val, y_val)}')

# create voting classifier
voting_classifer = VotingClassifier(estimators=model_list,voting='hard', n_jobs=-1)
voting_classifer.fit(x6_train, y_train)

# get accuracy (model to beat: RF with 0.8136 accuracy)
y_pred = voting_classifer.predict(x_val)

print("Training Score In Hard Voting and select the best model scores(DT& RF))",voting_classifer.score(x6_train, y_train))
print("Training Score In HRD Voting and select the best model scores(DT& RF))",voting_classifer.score(x6_val, y_val))

# classification report for Voting 
print(classification_report(y_val, y_pred, digits=3, zero_division = 1))
acc_nb = accuracy_score(y_val, y_pred)
recall_nb = recall_score(y_val, y_pred, average="binary")
print("ACCURACY:",accuracy_score(y_val, y_pred))
print("RECALL:",recall_score(y_val, y_pred, average="binary"))

models_evalutions['Model'].append("VotingClassifier-Hard")
models_evalutions['Accuracy'].append(accuracy_score(y_val, y_pred))
models_evalutions['Recall'].append(recall_score(y_val, y_pred))
models_evalutions['Precision'].append(precision_score(y_val, y_pred))
models_evalutions['F1 score'].append(f1_score(y_val, y_pred))

# plotting confusion mtrix
cm = confusion_matrix(y_val, y_pred)
class_label = ["No-Diabetes", "Diabetes"]
df_cm = pd.DataFrame(cm, index = class_label, columns = class_label)
sns.heatmap(df_cm, annot = True, fmt = "d", cmap='Blues')
plt.title('Confusion matrix for Voting', fontsize = 20); # title with fontsize 20

"""## Experiment 6-2: Ensembling with Average Voting"""

x7_train=x_train.copy()
x7_val=x_val.copy()

# create voting classifier
voting_classifer1 = VotingClassifier(estimators=model_list,voting='soft', n_jobs=-1)
voting_classifer1.fit(x7_train, y_train)

print("Training Score In Average Voting and select the best model scores(DT& RF))",voting_classifer1.score(x7_train, y_train))
print("Training Score In Avaerage Voting and select the best model scores(DT& RF))",voting_classifer1.score(x7_val, y_val))

# Get accuracy (model to beat: RF with 0.8136 accuracy)
y_pred = voting_classifer1.predict(x7_val)

# classification report for voting avarage
print(classification_report(y_val, y_pred, digits=3, zero_division = 1))
acc_nb = accuracy_score(y_val, y_pred)
recall_nb = recall_score(y_val, y_pred, average="binary")
print("ACCURACY:",accuracy_score(y_val, y_pred))
print("RECALL:",recall_score(y_val, y_pred, average="binary"))

models_evalutions['Model'].append("VotingClassifier-Average Voting")
models_evalutions['Accuracy'].append(accuracy_score(y_val, y_pred))
models_evalutions['Recall'].append(recall_score(y_val, y_pred))
models_evalutions['Precision'].append(precision_score(y_val, y_pred))
models_evalutions['F1 score'].append(f1_score(y_val, y_pred))

# plotting confusion mtrix
cm = confusion_matrix(y_val, y_pred)
class_label = ["No-Diabetes", "Diabetes"]
df_cm = pd.DataFrame(cm, index = class_label, columns = class_label)
sns.heatmap(df_cm, annot = True, fmt = "d", cmap='Blues')
plt.title('Confusion matrix for Avarege Voting', fontsize = 20); # title with fontsize 20

"""## Experiment 6-3: Ensembling with Weighted Voting"""

x8_train=x_train.copy()
x8_val=x_val.copy()

# create voting classifier
weights = [1.5,1.5,1,1]
voting_model = VotingClassifier(estimators=model_list, voting='soft', weights = weights, n_jobs=-1)
voting_model.fit(x8_train, y_train)

# Get accuracy (model to beat: RF with 0.8136 accuracy)
y_pred = voting_model.predict(x8_val)

print("Training Score In Weighted Voting and select the best model scores(DT& RF))",voting_model.score(x7_train, y_train))
print("Training Score In Weighted Voting and select the best model scores(DT& RF))",voting_model.score(x7_val, y_val))

# classification report for Naive Bayes
print(classification_report(y_val, y_pred, digits=3, zero_division = 1))
acc_nb = accuracy_score(y_val, y_pred)
recall_nb = recall_score(y_val, y_pred, average="binary")
print("ACCURACY:",accuracy_score(y_val, y_pred))
print("RECALL:",recall_score(y_val, y_pred, average="binary"))

models_evalutions['Model'].append("VotingClassifier-Weighted Voting")
models_evalutions['Accuracy'].append(accuracy_score(y_val, y_pred))
models_evalutions['Recall'].append(recall_score(y_val, y_pred))
models_evalutions['Precision'].append(precision_score(y_val, y_pred))
models_evalutions['F1 score'].append(f1_score(y_val, y_pred))

# plotting confusion mtrix
cm = confusion_matrix(y_val, y_pred)
class_label = ["No-Diabetes", "Diabetes"]
df_cm = pd.DataFrame(cm, index = class_label, columns = class_label)
sns.heatmap(df_cm, annot = True, fmt = "d", cmap='Blues')
plt.title('Confusion matrix for Wieghted Voting', fontsize = 20); # title with fontsize 20

"""# Expreiment 7: Bagging"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import BaggingClassifier
tree_clas = DecisionTreeClassifier(random_state=42)

bag_model = BaggingClassifier(
    base_estimator = DecisionTreeClassifier(),
    n_estimators = 100,
    max_samples = 0.8,
    oob_score=True,
    random_state = 0
)

bag_model.fit(x_train, y_train)

bag_model.oob_score_

bag_model.score(x_val,y_val)

y_pred = tree.predict(x_val)

print("DT Accuracy=",accuracy_score(y_val, y_pred))
print("DT F1 score=",f1_score(y_val, y_pred))

# classification report Baggin
print(classification_report(y_val, y_pred, digits=3, zero_division = 1))
acc_nb = accuracy_score(y_val, y_pred)
recall_nb = recall_score(y_val, y_pred, average="binary")
print("ACCURACY:",accuracy_score(y_val, y_pred))
print("RECALL:",recall_score(y_val, y_pred, average="binary"))

models_evalutions['Model'].append(" BaggingClassifier")
models_evalutions['Accuracy'].append(accuracy_score(y_val, y_pred))
models_evalutions['Recall'].append(recall_score(y_val, y_pred))
models_evalutions['Precision'].append(precision_score(y_val, y_pred))
models_evalutions['F1 score'].append(f1_score(y_val, y_pred))

# plotting confusion mtrix
cm = confusion_matrix(y_val, y_pred)
class_label = ["No-Diabetes", "Diabetes"]
df_cm = pd.DataFrame(cm, index = class_label, columns = class_label)
sns.heatmap(df_cm, annot = True, fmt = "d", cmap='Blues')
plt.title('Confusion matrix for BaggingClassifier', fontsize = 20); # title with fontsize 20

"""# Expreiment 8 : Naive Bayes"""







# classification report for Naive Bayes



# plotting confusion mtrix
cm = confusion_matrix(y_val, y_pred)
class_label = ["No-Diabetes", "Diabetes"]
df_cm = pd.DataFrame(cm, index = class_label, columns = class_label)
sns.heatmap(df_cm, annot = True, fmt = "d", cmap='Blues')
plt.title('Confusion matrix forGaussianNB', fontsize = 20); # title with fontsize 20

result= pd.DataFrame.from_dict(models_evalutions)
result

cmap = sns.diverging_palette(220, 10, as_cmap=True)

result.plot.bar(x='Model',y=['Accuracy','Precision','Precision','Recall','F1 score'], cmap='Blues_r', figsize=(10,10))
plt.title("Comparison of Scores of the Models")
plt.ylabel("Models")
plt.xlabel("ML Algorithms's scores in Diabetes Dataset")
plt.show()

"""# Deplyment """

import warnings
import pickle
from sklearn.pipeline import make_pipeline
warnings.filterwarnings("ignore")

lg = LogisticRegression()
lg.fit(x_train, y_train)

def Predictor(input_data):
  v1=voting_classifer.predict(input_data)
  v2=voting_classifer1.predict(input_data)
  v3=voting_model.predict(input_data)
  print(v1,v2,v3)
  if v1+v2+v3 >=2:
    return 1
  else:
    return 0


data=(1,1,40,0,0,5,18,15,1,0,9)
arr=np.asarray(data)
arr=arr.reshape(1,-1)
ans=Predictor(arr)
print(ans)

from flask import Flask
from flask_restful import Api,Resource,reqparse

ml_put_args=reqparse.RequestParser()
ml_put_args.add_argument("bp",type=int)
ml_put_args.add_argument("chol",type=int)
ml_put_args.add_argument("bmi",type=int)
ml_put_args.add_argument("heart",type=int)
ml_put_args.add_argument("phy",type=int)
ml_put_args.add_argument("genhlth",type=int)
ml_put_args.add_argument("menthlth",type=int)
ml_put_args.add_argument("diffwalk",type=int)
ml_put_args.add_argument("sex",type=int)
ml_put_args.add_argument("age",type=int)


app =Flask(__name__)
api=Api(app)
class MachineLearning(Resource):
    def get(self):
        return {"data":"hello"}
    def post(self):
       args=ml_put_args.parse_args()
       print(args)
       return {"data":"This is post request"}
api.add_resource(MachineLearning,"/")
if __name__=="__main__":
    app.run(debug=True)